<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"> <channel> <title>Harmen Stoppels</title> <description></description> <link>http://stoppels.blog</link> <atom:link href="http://stoppels.blog/feed.xml" rel="self" type="application/rss+xml"/> <item> <title>GSOC 2017 summary</title> <description>As GSOC 2017 is coming to an end, I'd like to use this blog post to give a summary of the things I have been working on.</description> <pubDate>Fri, 18 Aug 2017 00:00:00 +0200</pubDate> <link>http://stoppels.blog/posts/gsoc-2017-summary</link> <guid isPermaLink="true">http://stoppels.blog/posts/gsoc-2017-summary</guid> </item> <item> <title>Orthogonalization performance</title> <description>Methods like GMRES and Jacobi-Davidson construct an orthogonal basis for their search subspace. It is well-known that classical Gram-Schmidt (CGS) is vulnerable for loss of orthogonality due to rounding errors. Modified Gram-Schmidt (MGS) is usually the fix for this, yet it is not free of rounding errors and is memory-bound when it comes to performance. In this post we'll look into iterative or refined orthogonalization methods and their performance.</description> <pubDate>Sun, 18 Jun 2017 00:00:00 +0200</pubDate> <link>http://stoppels.blog/posts/orthogonalization-performance</link> <guid isPermaLink="true">http://stoppels.blog/posts/orthogonalization-performance</guid> </item> <item> <title>Jacobi-Davidson example</title> <description>Let's take a look how the Jacobi-Davidson algorithm targets interior eigenvalues of a Poisson matrix.</description> <pubDate>Sun, 11 Jun 2017 00:00:00 +0200</pubDate> <link>http://stoppels.blog/posts/jacobi-davidson-example</link> <guid isPermaLink="true">http://stoppels.blog/posts/jacobi-davidson-example</guid> </item> <item> <title>Harmonic Ritz values visualized</title> <description>A hopefully insightful visualizion of harmonic Ritz values versus ordinary Ritz values.</description> <pubDate>Wed, 07 Jun 2017 00:00:00 +0200</pubDate> <link>http://stoppels.blog/posts/harmonic-ritz-values-visualized</link> <guid isPermaLink="true">http://stoppels.blog/posts/harmonic-ritz-values-visualized</guid> </item> <item> <title>Interior eigenvalues in Jacobi-Davidson</title> <description>From the Arnoldi method we know that Ritz values tend to converge to exterior eigenvalues; similar behaviour is observed in Jacobi-Davidson. This is an issue when restarting the method: a Ritz value close to a specified interior target is not necessarily a good approximation of an eigenvalue in that neighbourhood; the Ritz value might as well be on its way to an exterior eigenvalue, and in that case we would not want the Ritz vector to be retained at restart. An alternative is to use harmonic Ritz pairs.</description> <pubDate>Tue, 06 Jun 2017 00:00:00 +0200</pubDate> <link>http://stoppels.blog/posts/interior-eigenvalues</link> <guid isPermaLink="true">http://stoppels.blog/posts/interior-eigenvalues</guid> </item> <item> <title>Towards the next eigenpair in Jacobi-Davidson</title> <description>After removing a converged Ritz vector from the search space, we must ensure that new expansions do not bring it back. In practice this means updating the correction equation so that these directions are deflated.</description> <pubDate>Sat, 03 Jun 2017 00:00:00 +0200</pubDate> <link>http://stoppels.blog/posts/towards-the-next-eigenpair</link> <guid isPermaLink="true">http://stoppels.blog/posts/towards-the-next-eigenpair</guid> </item> <item> <title>Non-Hermetian eigenproblems in Jacobi-Davidson</title> <description>Non-Hermetian matrices do not necessarily have orthogonal eigenvectors for distinct eigenvalues. So far we have relied upon this property when shrinking the search subspace. Fortunately we can work with the Schur decomposition to tackle this problem.</description> <pubDate>Sat, 03 Jun 2017 00:00:00 +0200</pubDate> <link>http://stoppels.blog/posts/non-hermetian-eigenproblems</link> <guid isPermaLink="true">http://stoppels.blog/posts/non-hermetian-eigenproblems</guid> </item> <item> <title>Shrinking the search subspace in Jacobi-Davidson</title> <description>Once an approximate eigenvector has converged, we must remove it from the search space, so that other eigenpairs can be found. Also, when a search subspace becomes too large, a restart is necessary. This also requires removing vectors from the search space. Fortunately, shrinking the search subspace is easier in comparison to IRAM.</description> <pubDate>Fri, 02 Jun 2017 00:00:00 +0200</pubDate> <link>http://stoppels.blog/posts/shrinking-the-search-subspace</link> <guid isPermaLink="true">http://stoppels.blog/posts/shrinking-the-search-subspace</guid> </item> <item> <title>Options in Jacobi-Davidson</title> <description>In my previous post I showed the defining property for Jacobi-Davidson, but it turns out that there is a lot of freedom in the remaining parts of the algorithm. For instance in how to initialize it; this post will be about exactly that.</description> <pubDate>Fri, 02 Jun 2017 00:00:00 +0200</pubDate> <link>http://stoppels.blog/posts/options-in-jacobi-davidson</link> <guid isPermaLink="true">http://stoppels.blog/posts/options-in-jacobi-davidson</guid> </item> <item> <title>Jacobi-Davidson</title> <description>As part of Google Summer of Code I'm implementing several iterative methods for linear systems of equations and eigenvalue problems in Julia. Up to now I have studied Jacobi-Davidson and have a poor man's version working. The first post is about my notes on the method.</description> <pubDate>Thu, 01 Jun 2017 00:00:00 +0200</pubDate> <link>http://stoppels.blog/posts/jacobi-davidson</link> <guid isPermaLink="true">http://stoppels.blog/posts/jacobi-davidson</guid> </item> </channel> </rss>