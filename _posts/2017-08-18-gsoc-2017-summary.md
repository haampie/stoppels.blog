---
layout: post
title: "GSOC 2017 summary"
description: "As GSOC 2017 is coming to an end, I'd like to use this blog post to give a summary of the things I have been working on."
tags: [gsoc, julia]
---

The central part of my GSOC project concerns implementing the Jacobi-Davidson method natively in Julia, available in [JacobiDavidson.jl](https://github.com/haampie/JacobiDavidson.jl). This method computes a few approximate solutions of the eigenvalue problem {%katex%}Ax = \lambda Bx{% endkatex %} for large matrices {%katex%}A{%endkatex%} and {%katex%}B{%endkatex%}. As it uses iterative solvers internally, much time has gone into improving [IterativeSolvers.jl](https://github.com/JuliaMath/IterativeSolvers.jl) in general. Lastly, as iterative solvers are typically used with preconditioners, I have implemented the incomplete LU factorization for sparse matrices as well in [ILU.jl](https://github.com/haampie/ILU.jl).

## [JacobiDavidson.jl](https://github.com/haampie/JacobiDavidson.jl)
The Jacobi-Davidson implementation is ready for use and can be applied to solving the (generalized) eigenvalue problem for non-Hermitian matrices. It's similar to the `eigs` method already available in Julia: you can get a couple eigenvalues near a specified target in the complex plane.

The `jdqr` function computes a partial Schur decomposition {% katex %}AQ = QR {% endkatex %} of a general matrix {% katex %}A{% endkatex %}. The columns of {% katex %}Q{% endkatex %} are orthonormal Schur vectors, and the diagonal of {% katex %}R{% endkatex %} contains the eigenvalues of {% katex %}A{% endkatex %}. Working with Schur vectors is numerically more stable than working with eigenvectors, but eigenvectors can be computed from the Schur vectors when necessary.

For the generalized problem we have the `jdqz` function that computes a partial generalized Schur decomposition {% katex %}AZ = QS{% endkatex %} and {% katex %}BZ = QT{% endkatex %} for a matrix pencil {% katex %}(A, B){% endkatex %}. Here {% katex %}Q{% endkatex %} and {% katex %}Z{% endkatex %} have orthonormal columns and {% katex %}S{% endkatex %} and {% katex %}T{% endkatex %} upper triangular. The eigenvalues {%katex%}\lambda_k{%endkatex%} of {% katex %}Ax = \lambda_k Bx{% endkatex %} can be found by computing {% katex %}\lambda_k = \frac{S_{kk}}{T_{kk}}{% endkatex %}.

At this point no official release has been tagged yet, as there is still some work to be done: hopefully the `jdqr` and `jdqz` methods can largely be merged as they are very similar; there are some optimizations for Hermitian problems that aren't yet implemented; and lastly the methods do not yet support generic vectors and numbers.

For an example of `jdqz` see the last section of this post.

## [IterativeSolvers.jl](https://github.com/JuliaMath/IterativeSolvers.jl)
My contributions to IterativeSolvers.jl include 
- improving speed and memory usage in [GMRES](https://github.com/JuliaMath/IterativeSolvers.jl/pull/131), [CG](https://github.com/JuliaMath/IterativeSolvers.jl/pull/130), [Chebyshev iteration](https://github.com/JuliaMath/IterativeSolvers.jl/pull/145), the [power method](https://github.com/JuliaMath/IterativeSolvers.jl/pull/139) and stationary methods mentioned below.
- adding missing iterative methods [MINRES](https://github.com/JuliaMath/IterativeSolvers.jl/pull/148) and [BiCGStab(l)](https://github.com/JuliaMath/IterativeSolvers.jl/pull/134);
- implementing stationary methods for sparse matrices [#156](https://github.com/JuliaMath/IterativeSolvers.jl/pull/156);
- upgrading the package to Julia 0.6 and thereby [updating the tests](https://github.com/JuliaMath/IterativeSolvers.jl/pull/150);
- improving and restructuring [the documentation](https://github.com/JuliaMath/IterativeSolvers.jl/pull/157).

## [ILU.jl](https://github.com/haampie/ILU.jl)
Computing the full LU decomposition of a large, sparse matrix requires too much memory and computational work. By simply dropping small terms during the LU factorization these problems can be remedied, at the expense of being inexact or incomplete (ILU). This ILU factorization can still be an excellent preconditioner to speed up iterative methods.

As ILU for the `SparseMatrixCSC` type was not yet available in Julia, I've implemented it based on the article "Crout versions of ILU for general sparse matrices" by Na Li, Yousef Saad and Edmond Chow.

The package is completely ready for use and is well tested. See below for an example.

In a later version the package can be improved by adding different dropping strategies.

## Examples
Below you can find a few examples on how to use the packages I've been working on.

### Jacobi-Davidson
Let's take a look at a toy example of the generalized eigenvalue problem {% katex %}Ax = \lambda Bx{% endkatex %} where {% katex %}A{% endkatex %} and {% katex %}B{% endkatex %} are diagonal matrices of size {% katex %}n \times n{% endkatex %} with {% katex %}A_{kk} = \sqrt{k}{% endkatex %} and {% katex %}B_{kk} = 1 / \sqrt{k}{% endkatex %}. The eigenvalues are just the integers {% katex %}1, \cdots, n{% endkatex %}.

We implement the action of the matrices {% katex %}A{% endkatex %} and {% katex %}B{% endkatex %} matrix-free, using LinearMaps.jl:

{% highlight julia %}
using LinearMaps

function myA!(y, x)
  for i = 1 : length(x)
    @inbounds y[i] = sqrt(i) * x[i]
  end
end

function myB!(y, x)
  for i = 1 : length(x)
    @inbounds y[i] = x[i] / sqrt(i)
  end
end

A = LinearMap{Complex128}(myA!, n; ismutating = true)
B = LinearMap{Complex128}(myB!, n; ismutating = true)
{% endhighlight %}

Next we fix {% katex %}n = 100{% endkatex %}, set the number of requested eigenpairs to 5 and target the eigenvalues with largest real part, somewhere near {% katex %}110.0{% endkatex %}. We use a couple steps of BiCGStab(2) as an internal iterative solver, and set the dimension of the search space from 5 to 10 vectors:

{% highlight julia %}
using JacobiDavidson

n = 100
target = LR(110.0 + 0.0im) # Largest Real part

schur, resnorms = jdqz(A, B, 
    bicgstabl_solver(n, max_mv_products = 10, l = 2),
    target = target,
    pairs = 5,
    ɛ = 1e-9,
    min_dimension = 5,
    max_dimension = 10,
    max_iter = 100,
    verbose = true
)

println.(schur.alphas ./ schur.betas)
plot(resnorms, yscale = :log10, marker = :+, label = "Residual norm", xlabel = "Iteration")
{% endhighlight %}

It indeed finds the eigenvalues from 96 up to 100:

{% highlight text %}
100.00000000000041 + 5.329070518200757e-14im
98.99999999999996 - 1.7674527390216706e-14im
98.00000000000013 - 3.542337114695263e-14im
97.00000000000024 + 1.749508591650104e-14im
96.00000000000013 + 1.3053503572900918e-14im
{% endhighlight %}

The plot shows the convergence history, which is the residual norm {% katex %}\|Ax - \lambda Bx\|{% endkatex %} in each iteration:

<figure class="full_width_fig">
    {% img summary/residualnorm.svg %}
</figure>

#### Preconditioning
The problem gets harder when we increase {% katex %}n{% endkatex %} a few orders of magnitude. Especially if we want to target eigenvalues in the interior of the spectrum (near {% katex %}n / 2{% endkatex %}), GMRES and BiCGStab(l) might have trouble solving a very indefinite system.

In that case a preconditioner for {% katex %}(A - \tau B){% endkatex %} can be used, where {% katex %}\tau{% endkatex %} is the target. We will just use the exact inverse, which is a diagonal matrix {% katex %}P{% endkatex %} with entries {% katex %}P_{kk} = \sqrt{k} / (k - \tau){% endkatex %}.

It can be implemented matrix-free and in-place:

{% highlight julia %}
import Base.LinAlg.A_ldiv_B!

struct SuperPreconditioner{numT <: Number}
    target::numT
end

function A_ldiv_B!(p::SuperPreconditioner, x)
    for i = 1 : length(x)
        @inbounds x[i] *= sqrt(i) / (i - p.target)
    end
end
{% endhighlight %}

Now we call Jacobi-Davidson with the `Near` target and pass the preconditioner. This time we use GMRES, but BiCGStab works as well.

{% highlight julia %}
n = 100_000
τ = 50_000.1 + 0im
target = Near(τ)
A = LinearMap{Complex128}(myA!, n; ismutating = true)
B = LinearMap{Complex128}(myB!, n; ismutating = true)
P = SuperPreconditioner(τ)

schur, residuals = jdqz(A, B, 
    gmres_solver(n, iterations = 10),
    preconditioner = P,
    target = target,
    pairs = 5,
    ɛ = 1e-9,
    min_dimension = 5,
    max_dimension = 10,
    max_iter = 200,
    verbose = true
)
{% endhighlight %}

It converges to the eigenvalues 49999, 50000, 50001, 50002 and 50004:

{% highlight julia %}
50004.00000000014 + 3.5749921718300463e-12im
49999.999999986496 - 7.348301591250897e-12im
50001.00000000359 - 1.9761169705101647e-11im
49998.99999999998 - 1.0866253642291695e-10im
50002.00000000171 - 2.3559720511618024e-11im
{% endhighlight %}

It does not yet detect 50003, but that might happen when `pairs` is increased a bit. What is more interesting is that the total number of iterations is small as a result of the preconditioner:

<figure class="full_width_fig">
    {% img summary/residualnorm_2.svg %}
</figure>

It's not easy to construct a preconditioner this good for any given problem, but usually people tend to know what works well in specific classes of problems. If no specific preconditioner is availabe, you can always try a general one such as ILU. The next section illustrates that.

### ILU example
As an example of how ILU can be used we generate a non-symmetric, banded matrix having a structure that typically arises in finite differences schemes of three-dimensional problems:

{% highlight julia %}
n = 64
N = n^3
A = spdiagm((fill(-1.0, n - 1), fill(3.0, n), fill(-2.0, n - 1)), (-1, 0, 1))
Id = speye(n)
A = kron(A, Id) + kron(Id, A)
A = kron(A, Id) + kron(Id, A)
x = ones(N)
b = A * x
{% endhighlight %}

The matrix {% katex %}A{% endkatex %} has size {% katex %}64^3 \times 64^3{% endkatex %}. We want to solve the problem {% katex %}Ax = b{% endkatex %} using for instance BiCGStab(2), but it turns out that convergence can get slow when the size of the problem grows. A quick benchmark shows it takes about 2.0 seconds to solve the problem to a reasonable tolerance: 

{% highlight julia %}
> using BenchmarkTools, IterativeSolvers
> my_x = @btime bicgstabl($A, $b, 2, max_mv_products = 2000);
2.051 s
> norm(b - A * my_x) / norm(b)
1.6967043606691152e-9
{% endhighlight %}

Now let's construct the incomplete LU decomposition:

{% highlight julia %}
> using ILU
> LU = crout_ilu(A, τ = 0.1)
> nnz(LU) / nnz(A)
2.1180353639352374
{% endhighlight %}

Using the above drop tolerance {% katex %}\tau{% endkatex %}, we get an approximate LU decomposition using only about twice as many entries as the original matrix, which is reasonable. Let's see what happens when we benchmark the solver again, now with ILU as a preconditioner:

{% highlight julia %}
> my_x = @btime bicgstabl($A, $b, 2, Pl = $LU, max_mv_products = 2000);
692.187 ms
> norm(b - A * my_x) / norm(b)
2.133397068536056e-9
{% endhighlight %}

It solves the problem 66% faster to the same tolerance. There is of course a caveat, as constructing the preconditioner itself takes time as well:

{% highlight julia %}
> LU = @btime crout_ilu($A, τ = 0.1);
611.019 ms
{% endhighlight %}

So all in all the problem is solved about 36% faster. However, if we have multiple right-hand sides for the same matrix, we can construct the preconditioner only once and use it multiple times. Even when the matrix changes slightly you could reuse the ILU factorization. The latter is exactly what happens in Jacobi-Davidson.